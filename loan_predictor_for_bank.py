# -*- coding: utf-8 -*-
"""Loan Predictor for Bank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E4n7TGHArhD6l-zrx5HbLiBmuyBZ832f
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import pickle
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import OneHotEncoder,LabelEncoder,StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,precision_score,recall_score,f1_score,roc_auc_score,roc_curve
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier
from sklearn.model_selection import cross_val_score,RandomizedSearchCV,GridSearchCV
warnings.filterwarnings('ignore')

import sklearn
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

"""We are importing pandas library and acessing dataset with help of pd.read_csv(path) functions from google drive ."""

import  pandas as pd
df = pd.read_csv(r'/content/drive/MyDrive/Olu/bankloan.csv', encoding= 'unicode_escape')

print("Shape of Dataset ",df.shape)

df['Credit_History']=df['Credit_History'].astype(bool)

df.head(2)

df.tail(2)

df.dtypes

df.info()

statistics_of_data = []
for col in df.columns:
  statistics_of_data.append((col,
                             df[col].nunique(),
                             df[col].isnull().sum()*100/df.shape[0],

                             df[col].dtype
                             ))
stats_df = pd.DataFrame(statistics_of_data, columns=['Feature', 'Uniq_val', 'missing_val', 'type'])

stats_df

cross_tab = pd.crosstab(df['Education'], df['Status'])
cross_tab

def make_plots(feature, title="", limited=False, n=10):
    print("Total unique values are: ", len(feature.value_counts()), "\n\n")
    print("Category\tValue\n")
    if limited:
        data = feature.value_counts()[0:n]
    else:
        data = feature.value_counts()
    print(data)
    categories_num = len(data)
    #plotting bar-plot and pie chart
    sns.set_style('darkgrid')
    plt.figure(figsize=(16,5))
    plt.subplot(1,2,1)
    plt.title(title, fontsize=16)
    plt.xticks(rotation=45)
    plot = sns.barplot(x=data.index, y=data.values, edgecolor="white", palette=sns.palettes.color_palette("icefire"))
    total = len(feature)
    for p in plot.patches:
        percentage = '{:.1f}%'.format(100 * p.get_height()/total)
        x = p.get_x() + p.get_width() / 2 - 0.08
        y = p.get_y() + p.get_height()
        plot.annotate(percentage, (x, y), size = 12)

    plt.subplot(1,2,2)
    labels = data.index
    plt.pie(x=data, autopct="%.1f%%", explode=[0.02]*categories_num, labels=labels, pctdistance=0.5)
    plt.title(title, fontsize=16)
    plt.show()

def make_plots_xy(x, y, title="", xlable="", ylable="", palette="Blues_d"):
    #plotting bar-plot and pie chart
    sns.set_style('darkgrid')
    plt.figure(figsize=(16,5))
    plt.subplot(1,2,1)
    sns.barplot(x, y, palette=palette)
    plt.title(title, fontsize=14)
    plt.xlabel(xlable, fontsize=12)
    plt.ylabel(ylable, fontsize=12)
    plt.xticks(rotation=65)

    plt.subplot(1,2,2)
    categories_num = len(x)
    plt.pie(x=y, autopct="%.1f%%", explode=[0.08]*categories_num, labels=x, pctdistance=0.5)
    plt.title(title, fontsize=14)
    plt.show()

make_plots(df.Education)

make_plots(df.Status)

make_plots(df.Credit_History)

from scipy.stats import chi2_contingency
cross_tab = pd.crosstab(df['Education'], df['Status'])
chi2, p, dof, expected = chi2_contingency(cross_tab)

# Set the significance level (alpha)
alpha = 0.05

# Print the results
print("Chi-squared statistic:", chi2)
print("P-value:", p)
print("Degrees of freedom:", dof)
print("Expected frequencies table:")
print(expected)

# Interpret the results
if p < alpha:
    print("Reject the null hypothesis: There is a significant association between education level and loan approval.")
else:
    print("Fail to reject the null hypothesis: No significant association between education level and loan approval.")

from scipy.stats import chi2_contingency
cross_tab = pd.crosstab(df['Area'], df['Status'])
chi2, p, dof, expected = chi2_contingency(cross_tab)

# Set the significance level (alpha)
alpha = 0.05

# Print the results
print("Chi-squared statistic:", chi2)
print("P-value:", p)
print("Degrees of freedom:", dof)
print("Expected frequencies table:")
print(expected)

# Interpret the results
if p < alpha:
    print("Reject the null hypothesis: There is a significant association between Area and loan approval.")
else:
    print("Fail to reject the null hypothesis: No significant association between Area  and loan approval.")

from scipy.stats import chi2_contingency
chi2, p, dof, expected = chi2_contingency(cross_tab)

# Set the significance level (alpha)
alpha = 0.05

# Print the results
print("Chi-squared statistic:", chi2)
print("P-value:", p)
print("Degrees of freedom:", dof)
print("Expected frequencies table:")
print(expected)

# Interpret the results
if p < alpha:
    print("Reject the null hypothesis: There is a significant association between education level and loan approval.")
else:
    print("Fail to reject the null hypothesis: No significant association between education level and loan approval.")

df.shape

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder,LabelEncoder

from sklearn.ensemble import RandomForestClassifier

num_col = df.select_dtypes(exclude='object').columns

df.describe()

sns.scatterplot(x=df['Applicant_Income'],y=df['Loan_Amount'])

"""There are 2 ways to find the correlation b/w columns

1.Pearson Correlation


2.Spearman Correlation

It use the rank system to find out the correlation b/w columns due to which correlation is not affected by outliers
"""

# Pearson Correlation
plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0, linewidths=.5)
plt.title("Correlation Heatmap")
plt.show()
plt.show()

num_col

df['Credit_History'].isnull().sum()

num_col = list(num_col.values)
num_col.remove("Credit_History")

import warnings
import scipy.stats as stats

import warnings

warnings.simplefilter("ignore")
features = num_col
import seaborn as sns
sns.set_style('dark')
for col in features:
  try:

    plt.figure(figsize=(15,4))
    plt.subplot(131)
    sns.distplot(df[col], label="skew: " + str(np.round(df[col].skew(),2)))
    plt.legend()
    plt.subplot(132)
    sns.boxplot(df[col])
    plt.subplot(133)
    stats.probplot(df[col],dist="norm", plot=plt)
    plt.tight_layout()
    plt.show()
  except:
    pass

def iqr_capping(df, cols, factor):

    for col in cols:
      try:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)

        iqr = q3 - q1

        upper_whisker = q3 + (factor*iqr)
        lower_whisker = q1 - (factor*iqr)

        df[col] = np.where(
            df[col] > upper_whisker,
            upper_whisker,
            np.where(
        df[col] < lower_whisker,
        lower_whisker,
        df[col]))
      except:
        pass
    return df



df = iqr_capping(df, num_col, 1.5)

df['Status'] = LabelEncoder().fit_transform(df['Status'])

X= df.drop('Status',axis=1)
y=df['Status']
from sklearn.model_selection import train_test_split,cross_val_score
x_train, x_test, y_train, y_test = train_test_split(X,y, random_state = 42, test_size = 0.2)

x_train.isnull().sum()

x_train.info()

pd.crosstab(df.Self_Employed, df.Status).plot(kind='bar', stacked=True)
plt.show()

df.head(1)



from sklearn.pipeline import Pipeline,make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler,Normalizer
from sklearn.preprocessing import PowerTransformer,FunctionTransformer,OrdinalEncoder,OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest,chi2

# Impute missing values using train data

tns_na = ColumnTransformer(transformers=[
     ('median',SimpleImputer(strategy='median'),[5,7,8]),
     ('mode',SimpleImputer(strategy='most_frequent'),[0,1,2,3,4,6,9,10])

],remainder='passthrough')

tns_ohe = ColumnTransformer(transformers=[
        ('tnf2',OneHotEncoder(sparse=False,drop='first',handle_unknown='ignore'),[3,4,5,6,7,8,9,10]),

],remainder='passthrough')



import numpy as np

pt = PowerTransformer()

model = RandomForestClassifier()


pipe = Pipeline([
    ('trf_na',tns_na),
    ('trf_ohe',tns_ohe),

     ("pt",pt),

    ('model',model),

])
pipe

x_train.iloc[0]

pipe.fit(x_train,y_train)

from sklearn.metrics import accuracy_score
pred=pipe.predict(x_test)
acc=accuracy_score(y_test,pred)*100
print(acc)

pipe = Pipeline([
    ('trf_na',tns_na),
    ('trf_ohe',tns_ohe),
     ("pt",pt),
    ("m",model)
  ])

# pred=pipe.predict(x_test)
# acc=accuracy_score(y_test,pred)*100
# print(acc)
# # Display confusion matrix as a heatmap
# # Create confusion matrix
# cm = confusion_matrix(y_test, pred)
# plt.figure(figsize=(6, 6))
# sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
# plt.title("Confusion Matrix")
# plt.xlabel("Predicted Labels")
# plt.ylabel("True Labels")
# plt.show()

def cross_val(model):
    accuracies=cross_val_score(estimator=model,X=x_train,y=y_train,cv=10)
    return accuracies.mean()*100

def fit_evaluate(model,name):
    model.fit(x_train,y_train)
    y_pred=model.predict(x_test)
    cross=cross_val(model)
    a_s=accuracy_score(y_test,y_pred)*100
    pre_sc=precision_score(y_test,y_pred)*100
    rec_sc=recall_score(y_test,y_pred)*100
    f1_sc=f1_score(y_test,y_pred)*100
    roc_sc=roc_auc_score(y_test,y_pred)*100
    result=pd.DataFrame([[name,a_s,pre_sc,rec_sc,f1_sc,roc_sc]],columns=['model','accuracy','precision_score','recall_score','f1_score','roc_auc_score'])
    return result

from sklearn.svm import SVC
svc=SVC()

models=[("knn",KNeighborsClassifier()),("svc",SVC()),("rf",RandomForestClassifier())]
result_models=pd.DataFrame(columns=['model','accuracy','precision_score','recall_score','f1_score','roc_auc_score'])
for name,model in models:

  pipe = Pipeline([
    ('trf_na',tns_na),
    ('trf_ohe',tns_ohe),

     ("pt",pt),

    ('model',model),

  ])
  results = fit_evaluate(pipe,name)
  result_models=pd.concat([result_models,results])



result_models.sort_values(by='recall_score',ascending=False)



result_models[['model','accuracy',]]